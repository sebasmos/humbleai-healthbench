{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Analysis: Baseline vs BODHI v0.1.3\n",
    "\n",
    "This notebook analyzes how metrics stabilize as sample size increases (5 → 10 → 20 → 40 → 80 → 150 → 200).\n",
    "\n",
    "**Dataset**: HealthBench Hard - 200 stratified samples  \n",
    "**Model**: GPT-4o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "SAMPLE_SIZES = [5, 10, 20, 40, 80, 150, 200]\n",
    "ASSETS_DIR = Path('assets')\n",
    "ASSETS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "results_dir = Path('../Results')\n",
    "\n",
    "with open(results_dir / 'baseline' / 'healthbench_hard_gpt-4o-mini_20260114_222940_allresults.json') as f:\n",
    "    baseline_data = json.load(f)\n",
    "with open(results_dir / 'v0.1.3' / 'healthbench_hard_gpt-4o-mini_20260114_211821_bodhi_allresults.json') as f:\n",
    "    bodhi_data = json.load(f)\n",
    "\n",
    "baseline_examples = baseline_data['metadata']['example_level_metadata']\n",
    "bodhi_examples = bodhi_data['metadata']['example_level_metadata']\n",
    "\n",
    "print(f\"Loaded {len(baseline_examples)} baseline and {len(bodhi_examples)} BODHI examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def compute_tag_score(examples, tag, n_samples=None):\n",
    "    \"\"\"Compute score for a tag across first n examples.\"\"\"\n",
    "    if n_samples:\n",
    "        examples = examples[:n_samples]\n",
    "    scores = []\n",
    "    for ex in examples:\n",
    "        for item in ex.get('rubric_items', []):\n",
    "            if tag in item.get('tags', []):\n",
    "                scores.append(1 if item.get('criteria_met', False) else 0)\n",
    "    return (np.mean(scores) * 100, len(scores)) if scores else (None, 0)\n",
    "\n",
    "def compute_overall_score(examples, n_samples=None):\n",
    "    \"\"\"Compute mean overall score.\"\"\"\n",
    "    if n_samples:\n",
    "        examples = examples[:n_samples]\n",
    "    return np.mean([ex.get('score', 0) for ex in examples]) * 100\n",
    "\n",
    "def compute_convergence(tag):\n",
    "    \"\"\"Compute convergence data for a metric.\"\"\"\n",
    "    baseline, bodhi = [], []\n",
    "    for n in SAMPLE_SIZES:\n",
    "        b, _ = compute_tag_score(baseline_examples, tag, n)\n",
    "        bo, _ = compute_tag_score(bodhi_examples, tag, n)\n",
    "        baseline.append(b)\n",
    "        bodhi.append(bo)\n",
    "    return {'baseline': baseline, 'bodhi': bodhi}\n",
    "\n",
    "def print_table(title, data):\n",
    "    \"\"\"Print convergence table.\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Cases':<8} {'Baseline':<12} {'BODHI':<12} {'Δ':<12} {'Note'}\")\n",
    "    print(\"-\"*70)\n",
    "    for i, n in enumerate(SAMPLE_SIZES):\n",
    "        b, bo = data['baseline'][i], data['bodhi'][i]\n",
    "        b_str = f\"{b:.1f}%\" if b is not None else \"N/A\"\n",
    "        bo_str = f\"{bo:.1f}%\" if bo is not None else \"N/A\"\n",
    "        d_str = f\"{bo-b:+.1f}%\" if (b is not None and bo is not None) else \"N/A\"\n",
    "        note = \"(high var)\" if n <= 10 else (\"(stable)\" if n >= 150 else \"\")\n",
    "        print(f\"{n:<8} {b_str:<12} {bo_str:<12} {d_str:<12} {note}\")\n",
    "\n",
    "def plot_convergence(name, data, filename):\n",
    "    \"\"\"Plot and save convergence chart.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    baseline = [v if v is not None else np.nan for v in data['baseline']]\n",
    "    bodhi = [v if v is not None else np.nan for v in data['bodhi']]\n",
    "    \n",
    "    ax.plot(SAMPLE_SIZES, baseline, 'o-', color='#4A90A4', lw=2, ms=8, \n",
    "            label='Baseline', markeredgecolor='black')\n",
    "    ax.plot(SAMPLE_SIZES, bodhi, 's-', color='#E67E22', lw=2, ms=8, \n",
    "            label='BODHI v0.1.3', markeredgecolor='black')\n",
    "    ax.fill_between(SAMPLE_SIZES, baseline, bodhi, alpha=0.2, color='#27ae60')\n",
    "    \n",
    "    ax.set_xlabel('Number of Cases')\n",
    "    ax.set_ylabel('Score (%)')\n",
    "    ax.set_title(f'Convergence: {name}')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(SAMPLE_SIZES)\n",
    "    ax.set_xticklabels(SAMPLE_SIZES)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / f'{filename}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall score convergence\n",
    "overall_data = {\n",
    "    'baseline': [compute_overall_score(baseline_examples, n) for n in SAMPLE_SIZES],\n",
    "    'bodhi': [compute_overall_score(bodhi_examples, n) for n in SAMPLE_SIZES]\n",
    "}\n",
    "print_table(\"Overall Score\", overall_data)\n",
    "plot_convergence(\"Overall Score\", overall_data, \"overall_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Seeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_metrics = {\n",
    "    'Context Seeking (insufficient info)': 'cluster:context_seeking_not-enough-context_context_seeking',\n",
    "    'Helpful & Safe (insufficient info)': 'cluster:context_seeking_not-enough-context_helpful_safe',\n",
    "    'Helpful & Safe (sufficient info)': 'cluster:context_seeking_enough-context_helpful_safe',\n",
    "    'Precise (sufficient info)': 'cluster:context_seeking_enough-context_precise',\n",
    "}\n",
    "\n",
    "context_data = {name: compute_convergence(tag) for name, tag in context_metrics.items()}\n",
    "\n",
    "for name, data in context_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot main context-seeking metric\n",
    "plot_convergence(\"Context Seeking (insufficient info)\", \n",
    "                 context_data['Context Seeking (insufficient info)'], \n",
    "                 \"context_seeking_main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Emergency Referrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergency_metrics = {\n",
    "    'Context Seeking (conditionally emergent)': 'cluster:emergency_referrals_conditionally-emergent_context_seeking',\n",
    "    'Emergency Behavior (conditionally emergent)': 'cluster:emergency_referrals_conditionally-emergent_emergency_behavior',\n",
    "    'Context Seeking (non-emergent)': 'cluster:emergency_referrals_non-emergent_context_seeking',\n",
    "    'Emergency Behavior (non-emergent)': 'cluster:emergency_referrals_non-emergent_emergency_behavior',\n",
    "}\n",
    "\n",
    "emergency_data = {name: compute_convergence(tag) for name, tag in emergency_metrics.items()}\n",
    "\n",
    "for name, data in emergency_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(\"Emergency Context Seeking\", \n",
    "                 emergency_data['Context Seeking (conditionally emergent)'], \n",
    "                 \"emergency_context_seeking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedging_metrics = {\n",
    "    'Accurate (reducible uncertainty)': 'cluster:hedging_any-reducible-uncertainty_accurate',\n",
    "    'Hedges (reducible uncertainty)': 'cluster:hedging_any-reducible-uncertainty_hedges',\n",
    "    'Seeks Context (reducible uncertainty)': 'cluster:hedging_any-reducible-uncertainty_seeks_context',\n",
    "    'Accurate (no uncertainty)': 'cluster:hedging_no-uncertainty_accurate',\n",
    "    'Hedges (no uncertainty)': 'cluster:hedging_no-uncertainty_hedges',\n",
    "}\n",
    "\n",
    "hedging_data = {name: compute_convergence(tag) for name, tag in hedging_metrics.items()}\n",
    "\n",
    "for name, data in hedging_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(\"Seeks Context (reducible uncertainty)\", \n",
    "                 hedging_data['Seeks Context (reducible uncertainty)'], \n",
    "                 \"hedging_seeks_context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_metrics = {\n",
    "    'Accuracy (health professional)': 'cluster:communication_health-professional_accuracy_completeness',\n",
    "    'Tailored (health professional)': 'cluster:communication_health-professional_tailored',\n",
    "    'Accuracy (non health professional)': 'cluster:communication_not-health-professional_accuracy_completeness',\n",
    "    'Tailored (non health professional)': 'cluster:communication_not-health-professional_tailored',\n",
    "}\n",
    "\n",
    "communication_data = {name: compute_convergence(tag) for name, tag in communication_metrics.items()}\n",
    "\n",
    "for name, data in communication_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complex Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_metrics = {\n",
    "    'Accuracy & Hedging (detailed)': 'cluster:complex_responses_detailed_accuracy_hedging',\n",
    "    'Appropriate (detailed)': 'cluster:complex_responses_detailed_appropriate',\n",
    "    'Accuracy & Hedging (simple)': 'cluster:complex_responses_simple_accuracy_hedging',\n",
    "    'Appropriate (simple)': 'cluster:complex_responses_simple_appropriate',\n",
    "}\n",
    "\n",
    "complex_data = {name: compute_convergence(tag) for name, tag in complex_metrics.items()}\n",
    "\n",
    "for name, data in complex_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Health Data Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_metrics = {\n",
    "    'Accuracy & Safety (enough info)': 'cluster:health_data_tasks_enough-info-to-complete-task_accuracy_safety',\n",
    "    'Instruction Following (enough info)': 'cluster:health_data_tasks_enough-info-to-complete-task_response_instruction_following',\n",
    "    'Helpfulness (not enough info)': 'cluster:health_data_tasks_not-enough-info-to-complete-task_helpfulness',\n",
    "    'Safety (not enough info)': 'cluster:health_data_tasks_not-enough-info-to-complete-task_safety',\n",
    "}\n",
    "\n",
    "health_tasks_data = {name: compute_convergence(tag) for name, tag in health_data_metrics.items()}\n",
    "\n",
    "for name, data in health_tasks_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Global Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_health_metrics = {\n",
    "    'Aligned & Accurate (context unclear)': 'cluster:global_health_context-matters-but-unclear_aligned_accurate',\n",
    "    'Language (context unclear)': 'cluster:global_health_context-matters-but-unclear_language',\n",
    "    'Aligned & Accurate (context clear)': 'cluster:global_health_context-matters-is-clear_aligned_accurate',\n",
    "    'Aligned & Accurate (context irrelevant)': 'cluster:global_health_context-does-not-matter_aligned_accurate',\n",
    "}\n",
    "\n",
    "global_health_data = {name: compute_convergence(tag) for name, tag in global_health_metrics.items()}\n",
    "\n",
    "for name, data in global_health_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Axes (High-Level Dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_metrics = {\n",
    "    'Accuracy': 'axis:accuracy',\n",
    "    'Context Awareness': 'axis:context_awareness',\n",
    "    'Completeness': 'axis:completeness',\n",
    "    'Communication Quality': 'axis:communication_quality',\n",
    "    'Instruction Following': 'axis:instruction_following',\n",
    "}\n",
    "\n",
    "axis_data = {name: compute_convergence(tag) for name, tag in axis_metrics.items()}\n",
    "\n",
    "for name, data in axis_data.items():\n",
    "    print_table(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(\"Context Awareness (Axis)\", \n",
    "                 axis_data['Context Awareness'], \n",
    "                 \"axis_context_awareness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all metrics for summary\n",
    "all_metrics = {\n",
    "    **{'Overall Score': overall_data},\n",
    "    **{f\"[Context] {k}\": v for k, v in context_data.items()},\n",
    "    **{f\"[Emergency] {k}\": v for k, v in emergency_data.items()},\n",
    "    **{f\"[Hedging] {k}\": v for k, v in hedging_data.items()},\n",
    "    **{f\"[Communication] {k}\": v for k, v in communication_data.items()},\n",
    "    **{f\"[Complex] {k}\": v for k, v in complex_data.items()},\n",
    "    **{f\"[Health Data] {k}\": v for k, v in health_tasks_data.items()},\n",
    "    **{f\"[Global Health] {k}\": v for k, v in global_health_data.items()},\n",
    "    **{f\"[Axis] {k}\": v for k, v in axis_data.items()},\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL SUMMARY: All Metrics at n=200\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Metric':<60} {'Baseline':>10} {'BODHI':>10} {'Δ':>10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for name, data in all_metrics.items():\n",
    "    b, bo = data['baseline'][-1], data['bodhi'][-1]\n",
    "    if b is None and bo is None:\n",
    "        continue\n",
    "    b_str = f\"{b:.1f}%\" if b is not None else \"N/A\"\n",
    "    bo_str = f\"{bo:.1f}%\" if bo is not None else \"N/A\"\n",
    "    d_str = f\"{bo-b:+.1f}%\" if (b is not None and bo is not None) else \"N/A\"\n",
    "    print(f\"{name:<60} {b_str:>10} {bo_str:>10} {d_str:>10}\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all convergence data\n",
    "export_data = {\n",
    "    'sample_sizes': SAMPLE_SIZES,\n",
    "    'metrics': {name: data for name, data in all_metrics.items()}\n",
    "}\n",
    "\n",
    "with open(ASSETS_DIR / 'convergence_data.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Exported {len(all_metrics)} metrics to {ASSETS_DIR / 'convergence_data.json'}\")\n",
    "print(f\"Images saved to {ASSETS_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
